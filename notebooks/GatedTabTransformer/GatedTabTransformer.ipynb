{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import T_co\n",
    "notebook_idenifier = \"gated_transformer\"\n",
    "model_identifier = \"all_features\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GatedTabTransformer pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "def dropout_layers(layers, prob_survival):\n",
    "    if prob_survival == 1:\n",
    "        return layers\n",
    "\n",
    "    num_layers = len(layers)\n",
    "    to_drop = torch.zeros(num_layers).uniform_(0., 1.) > prob_survival\n",
    "\n",
    "    # make sure at least one layer makes it\n",
    "    if all(to_drop):\n",
    "        rand_index = randrange(num_layers)\n",
    "        to_drop[rand_index] = False\n",
    "\n",
    "    layers = [layer for (layer, drop) in zip(layers, to_drop) if not drop]\n",
    "    return layers\n",
    "\n",
    "\n",
    "def shift(t, amount, mask = None):\n",
    "    if amount == 0:\n",
    "        return t\n",
    "    return F.pad(t, (0, 0, amount, -amount), value = 0.)\n",
    "\n",
    "\n",
    "class PreShiftTokens(nn.Module):\n",
    "    def __init__(self, shifts, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.shifts = tuple(shifts)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.shifts == (0,):\n",
    "            return self.fn(x, **kwargs)\n",
    "\n",
    "        shifts = self.shifts\n",
    "        segments = len(shifts)\n",
    "        feats_per_shift = x.shape[-1] // segments\n",
    "        splitted = x.split(feats_per_shift, dim = -1)\n",
    "        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n",
    "        segments_to_shift = list(map(lambda args: shift(*args), zip(segments_to_shift, shifts)))\n",
    "        x = torch.cat((*segments_to_shift, *rest), dim = -1)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_inner, causal = False):\n",
    "        super().__init__()\n",
    "        self.scale = dim_inner ** -0.5\n",
    "        self.causal = causal\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim_in, dim_inner * 3, bias = False)\n",
    "        self.to_out = nn.Linear(dim_inner, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if self.causal:\n",
    "            mask = torch.ones(sim.shape[-2:], device = device).triu(1).bool()\n",
    "            sim.masked_fill_(mask[None, ...], -torch.finfo(q.dtype).max)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class SpatialGatingUnit(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_seq,\n",
    "        causal = False,\n",
    "        act = nn.Identity(),\n",
    "        heads = 1,\n",
    "        init_eps = 1e-3,\n",
    "        circulant_matrix = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_out = dim // 2\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.norm = nn.LayerNorm(dim_out)\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "        # parameters\n",
    "\n",
    "        if circulant_matrix:\n",
    "            self.circulant_pos_x = nn.Parameter(torch.ones(heads, dim_seq))\n",
    "            self.circulant_pos_y = nn.Parameter(torch.ones(heads, dim_seq))\n",
    "\n",
    "        self.circulant_matrix = circulant_matrix\n",
    "        shape = (heads, dim_seq,) if circulant_matrix else (heads, dim_seq, dim_seq)\n",
    "        weight = torch.zeros(shape)\n",
    "\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        init_eps /= dim_seq\n",
    "        nn.init.uniform_(self.weight, -init_eps, init_eps)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.ones(heads, dim_seq))\n",
    "\n",
    "    def forward(self, x, gate_res = None):\n",
    "        device, n, h = x.device, x.shape[1], self.heads\n",
    "\n",
    "        res, gate = x.chunk(2, dim = -1)\n",
    "        gate = self.norm(gate)\n",
    "\n",
    "        weight, bias = self.weight, self.bias\n",
    "\n",
    "        if self.circulant_matrix:\n",
    "            # build the circulant matrix\n",
    "\n",
    "            dim_seq = weight.shape[-1]\n",
    "            weight = F.pad(weight, (0, dim_seq), value = 0)\n",
    "            weight = repeat(weight, '... n -> ... (r n)', r = dim_seq)\n",
    "            weight = weight[:, :-dim_seq].reshape(h, dim_seq, 2 * dim_seq - 1)\n",
    "            weight = weight[:, :, (dim_seq - 1):]\n",
    "\n",
    "            # give circulant matrix absolute position awareness\n",
    "\n",
    "            pos_x, pos_y = self.circulant_pos_x, self.circulant_pos_y\n",
    "            weight = weight * rearrange(pos_x, 'h i -> h i ()') * rearrange(pos_y, 'h j -> h () j')\n",
    "\n",
    "        if self.causal:\n",
    "            weight, bias = weight[:, :n, :n], bias[:, :n]\n",
    "            mask = torch.ones(weight.shape[-2:], device = device).triu_(1).bool()\n",
    "            mask = rearrange(mask, 'i j -> () i j')\n",
    "            weight = weight.masked_fill(mask, 0.)\n",
    "\n",
    "        gate = rearrange(gate, 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        gate = einsum('b h n d, h m n -> b h m d', gate, weight)\n",
    "        gate = gate + rearrange(bias, 'h n -> () h n ()')\n",
    "\n",
    "        gate = rearrange(gate, 'b h n d -> b n (h d)')\n",
    "\n",
    "        if exists(gate_res):\n",
    "            gate = gate + gate_res\n",
    "\n",
    "        return self.act(gate) * res\n",
    "\n",
    "\n",
    "class gMLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_ff,\n",
    "        seq_len,\n",
    "        heads = 1,\n",
    "        attn_dim = None,\n",
    "        causal = False,\n",
    "        act = nn.Identity(),\n",
    "        circulant_matrix = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Sequential(\n",
    "            nn.Linear(dim, dim_ff),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.attn = Attention(dim, dim_ff // 2, attn_dim, causal) if exists(attn_dim) else None\n",
    "\n",
    "        self.sgu = SpatialGatingUnit(dim_ff, seq_len, causal, act, heads, circulant_matrix = circulant_matrix)\n",
    "        self.proj_out = nn.Linear(dim_ff // 2, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_res = self.attn(x) if exists(self.attn) else None\n",
    "        x = self.proj_in(x)\n",
    "        x = self.sgu(x, gate_res = gate_res)\n",
    "        x = self.proj_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class gMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        num_tokens = None,\n",
    "        dim,\n",
    "        depth,\n",
    "        seq_len,\n",
    "        heads = 1,\n",
    "        ff_mult = 4,\n",
    "        attn_dim = None,\n",
    "        prob_survival = 1.,\n",
    "        causal = False,\n",
    "        circulant_matrix = False,\n",
    "        shift_tokens = 0,\n",
    "        act = nn.Identity()\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (dim % heads) == 0, 'dimension must be divisible by number of heads'\n",
    "\n",
    "        dim_ff = dim * ff_mult\n",
    "        self.seq_len = seq_len\n",
    "        self.prob_survival = prob_survival\n",
    "\n",
    "        self.to_embed = nn.Embedding(num_tokens, dim) if exists(num_tokens) else nn.Identity()\n",
    "\n",
    "        token_shifts = tuple(range(0 if causal else -shift_tokens, shift_tokens + 1))\n",
    "        self.layers = nn.ModuleList([Residual(PreNorm(dim, PreShiftTokens(token_shifts, gMLPBlock(dim = dim, heads = heads, dim_ff = dim_ff, seq_len = seq_len, attn_dim = attn_dim, causal = causal, act = act, circulant_matrix = circulant_matrix)))) for i in range(depth)])\n",
    "\n",
    "        self.to_logits = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            Reduce('b n d -> b d', 'mean'),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_embed(x)\n",
    "        layers = self.layers if not self.training else dropout_layers(self.layers, self.prob_survival)\n",
    "        out = nn.Sequential(*layers)(x)\n",
    "        return self.to_logits(out)\n",
    "\n",
    "\n",
    "class gMLPClassification(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        patch_width,\n",
    "        seq_len,\n",
    "        num_classes,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads = 1,\n",
    "        ff_mult = 4,\n",
    "        attn_dim = None,\n",
    "        prob_survival = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (dim % heads) == 0, 'dimension must be divisible by number of heads'\n",
    "        num_patches = (seq_len // patch_width)\n",
    "\n",
    "        dim_ff = dim * ff_mult\n",
    "\n",
    "        self.to_patch_embed = nn.Sequential(\n",
    "            Rearrange('b (w p2) -> b (w) (p2)', p2 = patch_width),\n",
    "            nn.Linear(patch_width, dim)\n",
    "        )\n",
    "\n",
    "        self.prob_survival = prob_survival\n",
    "\n",
    "        self.layers = nn.ModuleList([Residual(PreNorm(dim, gMLPBlock(dim = dim, heads = heads, dim_ff = dim_ff, seq_len = num_patches, attn_dim = attn_dim))) for i in range(depth)])\n",
    "\n",
    "        self.to_logits = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            Reduce('b n d -> b d', 'mean'),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_patch_embed(x)\n",
    "        layers = self.layers if not self.training else dropout_layers(self.layers, self.prob_survival)\n",
    "        x = nn.Sequential(*layers)(x)\n",
    "        return self.to_logits(x)\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def pair(val):\n",
    "    return (val, val) if not isinstance(val, tuple) else val\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class HeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            heads=8,\n",
    "            dim_head=16,\n",
    "            dropout=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_tokens, dim, depth, heads, dim_head, attn_dropout, ff_dropout):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.Embedding(num_tokens, dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, HeadAttention(dim, heads=heads, dim_head=dim_head, dropout=attn_dropout))),\n",
    "                Residual(PreNorm(dim, FeedForward(dim, dropout=ff_dropout))),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeds(x)\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x)\n",
    "            x = ff(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, act=None):\n",
    "        super().__init__()\n",
    "        dims_pairs = list(zip(dims[:-1], dims[1:]))\n",
    "        layers = []\n",
    "        for ind, (dim_in, dim_out) in enumerate(dims_pairs):\n",
    "            is_last = ind >= (len(dims_pairs) - 1)\n",
    "            linear = nn.Linear(dim_in, dim_out)\n",
    "            layers.append(linear)\n",
    "\n",
    "            if is_last:\n",
    "                continue\n",
    "\n",
    "            act = default(act, nn.ReLU())\n",
    "            layers.append(act)\n",
    "\n",
    "        self.mlp = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i,lay in enumerate(self.mlp):\n",
    "            x = lay(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GatedTabTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            categories,\n",
    "            num_continuous,\n",
    "            transformer_dim,\n",
    "            transformer_depth,\n",
    "            transformer_heads,\n",
    "            transformer_dim_head=16,\n",
    "            dim_out=1,\n",
    "            mlp_depth=2,\n",
    "            select_dim = 128,\n",
    "            mlp_act=None,\n",
    "            num_special_tokens=2,\n",
    "            continuous_mean_std=None,\n",
    "            attn_dropout=0.,\n",
    "            ff_dropout=0.,\n",
    "            gmlp_enabled=False,\n",
    "            mlp_dimension=32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n",
    "\n",
    "        # categories related calculations\n",
    "\n",
    "        self.num_categories = len(categories)\n",
    "        self.num_unique_categories = sum(categories)\n",
    "\n",
    "        # create category embeddings table\n",
    "\n",
    "        self.num_special_tokens = num_special_tokens\n",
    "        total_tokens = self.num_unique_categories + num_special_tokens\n",
    "\n",
    "        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n",
    "\n",
    "        categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value=num_special_tokens)\n",
    "        categories_offset = categories_offset.cumsum(dim=-1)[:-1]\n",
    "        self.register_buffer('categories_offset', categories_offset)\n",
    "\n",
    "        # continuous\n",
    "\n",
    "        if exists(continuous_mean_std):\n",
    "            assert continuous_mean_std.shape == (num_continuous,\n",
    "                                                 2), f'continuous_mean_std must have a shape of ({num_continuous}, 2) where the last dimension contains the mean and variance respectively'\n",
    "        self.register_buffer('continuous_mean_std', continuous_mean_std)\n",
    "\n",
    "        self.norm = nn.LayerNorm(num_continuous)\n",
    "        self.num_continuous = num_continuous\n",
    "\n",
    "        # transformer\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            num_tokens=total_tokens,\n",
    "            dim=transformer_dim,\n",
    "            depth=transformer_depth,\n",
    "            heads=transformer_heads,\n",
    "            dim_head=transformer_dim_head,\n",
    "            attn_dropout=attn_dropout,\n",
    "            ff_dropout=ff_dropout\n",
    "        )\n",
    "\n",
    "        # mlp to logits\n",
    "\n",
    "        input_size = (transformer_dim * self.num_categories) + num_continuous\n",
    "\n",
    "        self.encoder = nn.Linear(input_size,select_dim)\n",
    "        self.decoder = nn.Linear(select_dim,input_size)\n",
    "\n",
    "        if gmlp_enabled:\n",
    "            self.mlp = gMLPClassification(\n",
    "                patch_width=1,\n",
    "                seq_len=input_size,\n",
    "                num_classes=dim_out,\n",
    "                dim=mlp_dimension,\n",
    "                depth=mlp_depth\n",
    "            )\n",
    "        else:\n",
    "            hidden_dimensions = []\n",
    "\n",
    "            for i in range(mlp_depth):\n",
    "                if mlp_dimension == -1:\n",
    "                    hidden_dimensions.append((input_size // 8) * (2 ** (mlp_depth - i)))\n",
    "                else:\n",
    "                    hidden_dimensions.append(mlp_dimension)\n",
    "\n",
    "            all_dimensions = [input_size, *hidden_dimensions, dim_out]\n",
    "            self.mlp = MLP(all_dimensions, act=mlp_act)\n",
    "\n",
    "    def forward(self, x_categ, x_cont=None):\n",
    "        assert x_categ.shape[\n",
    "                   -1] == self.num_categories, f'you must pass in {self.num_categories} values for your categories input'\n",
    "        x_categ += self.categories_offset\n",
    "\n",
    "        x = self.transformer(x_categ)\n",
    "\n",
    "        flat_categ = x.flatten(1)\n",
    "\n",
    "        if self.num_continuous != 0:\n",
    "            assert x_cont.shape[\n",
    "                       1] == self.num_continuous, f'you must pass in {self.num_continuous} values for your continuous input'\n",
    "\n",
    "            if exists(self.continuous_mean_std):\n",
    "                mean, std = self.continuous_mean_std.unbind(dim=-1)\n",
    "                x_cont = (x_cont - mean) / std\n",
    "\n",
    "            normed_cont = self.norm(x_cont)\n",
    "            x = torch.cat((flat_categ, normed_cont), dim=-1)\n",
    "        else:\n",
    "            x = flat_categ\n",
    "\n",
    "        embed = self.encoder(x)\n",
    "        x = self.decoder(embed)\n",
    "        return self.mlp(x), embed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def amex_metric_pytorch(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "\n",
    "    # convert dtypes to float64\n",
    "    y_true = y_true.double()\n",
    "    y_pred = y_pred.double()\n",
    "\n",
    "    # count of positives and negatives\n",
    "    n_pos = y_true.sum()\n",
    "    n_neg = y_pred.shape[0] - n_pos\n",
    "\n",
    "    # sorting by descring prediction values\n",
    "    indices = torch.argsort(y_pred, dim=0, descending=True)\n",
    "    preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "    # filter the top 4% by cumulative row weights\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum(dim=0)\n",
    "    four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "    # default rate captured at 4%\n",
    "    d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "    # weighted gini coefficient\n",
    "    lorentz = (target / n_pos).cumsum(dim=0)\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    # max weighted gini coefficient\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    # normalized weighted gini coefficient\n",
    "    g = gini / gini_max\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "with open(\"D:\\\\Academic_PC\\\\Sem 7\\\\Machine Learning\\\\MiniProject\\\\data\\\\xgb_preprocessed\\\\metadata.json\",\"r\") as f0:\n",
    "    config:dict = json.load(f0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "config[\"device\"] = \"gpu\"\n",
    "config[\"bs\"] = 128\n",
    "config[\"epoch\"] = 20\n",
    "config[\"train_shape\"] = (367131, 485)\n",
    "config[\"val_shape\"] = (91782, 485)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdevin-18\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.13.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.7"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>D:\\Academic_PC\\Sem 7\\Machine Learning\\MiniProject\\notebooks\\GatedTabTransformer\\wandb\\run-20230117_020854-3tvbru99</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/devin-18/GatedTabTransformer/runs/3tvbru99\" target=\"_blank\">misty-hill-1</a></strong> to <a href=\"https://wandb.ai/devin-18/GatedTabTransformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/devin-18/GatedTabTransformer/runs/3tvbru99?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x13177105040>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(config=config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if config['device'] == 'cpu':\n",
    "    device = \"cpu\"\n",
    "else:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train = pd.read_csv(\"../data/xgb_preprocessed/val_rows.csv\")\n",
    "#target = pd.read_csv('../data/train_labels.csv').target.values\n",
    "#test = pd.read_csv(\"../data/xgb_preprocessed/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def update_metadata(path,data:dict):\n",
    "    loaded_config = {}\n",
    "    if os. path.exists(path):\n",
    "        with open(path,\"r\") as f0:\n",
    "            loaded_config = json.load(f0)\n",
    "\n",
    "    for k,v in data.items():\n",
    "        loaded_config[k] = v\n",
    "\n",
    "    with open(path,\"w\") as f0:\n",
    "        json.dump(loaded_config,f0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "1314"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(359, 124)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(config.get(\"cont_features\")),len(config.get(\"cat_features\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class AmexDataset(Dataset):\n",
    "    def __init__(self,csv_path, cat_headers=config.get(\"cat_features\"),con_headers=config.get(\"cont_features\"),target_col=\"target\",bs=1):\n",
    "        self.csv_path = csv_path\n",
    "        self.bs = bs\n",
    "        self.csv = pd.read_csv(csv_path,usecols=[\"customer_ID\",target_col]+cat_headers+con_headers)\n",
    "        self.cat_headers = cat_headers\n",
    "        self.con_headers = con_headers\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cat_f = self.csv.loc[index,self.cat_headers]\n",
    "        con_f = self.csv[self.con_headers]\n",
    "        tar = self.csv[self.target_col]\n",
    "        return torch.from_numpy(np.int64(cat_f.to_numpy())),torch.from_numpy(np.float64(con_f.to_numpy())),torch.from_numpy(np.int8(tar.to_numpy()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "train_data = AmexDataset(\"D:\\\\Academic_PC\\\\Sem 7\\\\Machine Learning\\\\MiniProject\\\\data\\\\xgb_preprocessed\\\\train_rows.csv\",\n",
    "                         cat_headers=config.get(\"cat_features\"),\n",
    "                         con_headers=config.get(\"cont_features\"),shape=config[\"train_shape\"])\n",
    "val_data = AmexDataset(\"D:\\\\Academic_PC\\\\Sem 7\\\\Machine Learning\\\\MiniProject\\\\data\\\\xgb_preprocessed\\\\val_rows.csv\",\n",
    "                         cat_headers=config.get(\"cat_features\"),\n",
    "                         con_headers=config.get(\"cont_features\"),shape=config[\"val_shape\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GatedTabTransformer(\n",
    "    categories = tuple(config.get(\"cat_groups\")),      # tuple containing the number of unique values within each category\n",
    "    num_continuous = len(config.get(\"cont_features\")),                # number of continuous values\n",
    "    transformer_dim = 8,               # dimension, paper set at 32\n",
    "    dim_out = 1,                        # binary prediction, but could be anything\n",
    "    select_dim=128,\n",
    "    transformer_depth = 6,              # depth, paper recommended 6\n",
    "    transformer_heads = 4,              # heads, paper recommends 8\n",
    "    attn_dropout = 0.2,                 # post-attention dropout\n",
    "    ff_dropout = 0.2,                   # feed forward dropout\n",
    "    mlp_act = nn.LeakyReLU(0),          # activation for final mlp, defaults to relu, but could be anything else (selu, etc.)\n",
    "    mlp_depth=4,                        # mlp hidden layers depth\n",
    "    mlp_dimension=16,                   # dimension of mlp layers\n",
    "    gmlp_enabled=True                   # gmlp or standard mlp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "pred, embedding = model(torch.randint(1,5,(32,len(config.get(\"cat_groups\")))),torch.randn(32,len(config.get(\"cont_features\"))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 1])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 128])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "GatedTabTransformer(\n  (norm): LayerNorm((359,), eps=1e-05, elementwise_affine=True)\n  (transformer): Transformer(\n    (embeds): Embedding(5458, 8)\n    (layers): ModuleList(\n      (0): ModuleList(\n        (0): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): HeadAttention(\n              (to_qkv): Linear(in_features=8, out_features=192, bias=False)\n              (to_out): Linear(in_features=64, out_features=8, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n        )\n        (1): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): FeedForward(\n              (net): Sequential(\n                (0): Linear(in_features=8, out_features=64, bias=True)\n                (1): GEGLU()\n                (2): Dropout(p=0.2, inplace=False)\n                (3): Linear(in_features=32, out_features=8, bias=True)\n              )\n            )\n          )\n        )\n      )\n      (1): ModuleList(\n        (0): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): HeadAttention(\n              (to_qkv): Linear(in_features=8, out_features=192, bias=False)\n              (to_out): Linear(in_features=64, out_features=8, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n        )\n        (1): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): FeedForward(\n              (net): Sequential(\n                (0): Linear(in_features=8, out_features=64, bias=True)\n                (1): GEGLU()\n                (2): Dropout(p=0.2, inplace=False)\n                (3): Linear(in_features=32, out_features=8, bias=True)\n              )\n            )\n          )\n        )\n      )\n      (2): ModuleList(\n        (0): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): HeadAttention(\n              (to_qkv): Linear(in_features=8, out_features=192, bias=False)\n              (to_out): Linear(in_features=64, out_features=8, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n        )\n        (1): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): FeedForward(\n              (net): Sequential(\n                (0): Linear(in_features=8, out_features=64, bias=True)\n                (1): GEGLU()\n                (2): Dropout(p=0.2, inplace=False)\n                (3): Linear(in_features=32, out_features=8, bias=True)\n              )\n            )\n          )\n        )\n      )\n      (3): ModuleList(\n        (0): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): HeadAttention(\n              (to_qkv): Linear(in_features=8, out_features=192, bias=False)\n              (to_out): Linear(in_features=64, out_features=8, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n        )\n        (1): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): FeedForward(\n              (net): Sequential(\n                (0): Linear(in_features=8, out_features=64, bias=True)\n                (1): GEGLU()\n                (2): Dropout(p=0.2, inplace=False)\n                (3): Linear(in_features=32, out_features=8, bias=True)\n              )\n            )\n          )\n        )\n      )\n      (4): ModuleList(\n        (0): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): HeadAttention(\n              (to_qkv): Linear(in_features=8, out_features=192, bias=False)\n              (to_out): Linear(in_features=64, out_features=8, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n        )\n        (1): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): FeedForward(\n              (net): Sequential(\n                (0): Linear(in_features=8, out_features=64, bias=True)\n                (1): GEGLU()\n                (2): Dropout(p=0.2, inplace=False)\n                (3): Linear(in_features=32, out_features=8, bias=True)\n              )\n            )\n          )\n        )\n      )\n      (5): ModuleList(\n        (0): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): HeadAttention(\n              (to_qkv): Linear(in_features=8, out_features=192, bias=False)\n              (to_out): Linear(in_features=64, out_features=8, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n        )\n        (1): Residual(\n          (fn): PreNorm(\n            (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n            (fn): FeedForward(\n              (net): Sequential(\n                (0): Linear(in_features=8, out_features=64, bias=True)\n                (1): GEGLU()\n                (2): Dropout(p=0.2, inplace=False)\n                (3): Linear(in_features=32, out_features=8, bias=True)\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n  (encoder): Linear(in_features=1351, out_features=128, bias=True)\n  (decoder): Linear(in_features=128, out_features=1351, bias=True)\n  (mlp): gMLPClassification(\n    (to_patch_embed): Sequential(\n      (0): Rearrange('b (w p2) -> b (w) (p2)', p2=1)\n      (1): Linear(in_features=1, out_features=16, bias=True)\n    )\n    (layers): ModuleList(\n      (0): Residual(\n        (fn): PreNorm(\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          (fn): gMLPBlock(\n            (proj_in): Sequential(\n              (0): Linear(in_features=16, out_features=64, bias=True)\n              (1): GELU(approximate='none')\n            )\n            (sgu): SpatialGatingUnit(\n              (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n              (act): Identity()\n            )\n            (proj_out): Linear(in_features=32, out_features=16, bias=True)\n          )\n        )\n      )\n      (1): Residual(\n        (fn): PreNorm(\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          (fn): gMLPBlock(\n            (proj_in): Sequential(\n              (0): Linear(in_features=16, out_features=64, bias=True)\n              (1): GELU(approximate='none')\n            )\n            (sgu): SpatialGatingUnit(\n              (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n              (act): Identity()\n            )\n            (proj_out): Linear(in_features=32, out_features=16, bias=True)\n          )\n        )\n      )\n      (2): Residual(\n        (fn): PreNorm(\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          (fn): gMLPBlock(\n            (proj_in): Sequential(\n              (0): Linear(in_features=16, out_features=64, bias=True)\n              (1): GELU(approximate='none')\n            )\n            (sgu): SpatialGatingUnit(\n              (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n              (act): Identity()\n            )\n            (proj_out): Linear(in_features=32, out_features=16, bias=True)\n          )\n        )\n      )\n      (3): Residual(\n        (fn): PreNorm(\n          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n          (fn): gMLPBlock(\n            (proj_in): Sequential(\n              (0): Linear(in_features=16, out_features=64, bias=True)\n              (1): GELU(approximate='none')\n            )\n            (sgu): SpatialGatingUnit(\n              (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n              (act): Identity()\n            )\n            (proj_out): Linear(in_features=32, out_features=16, bias=True)\n          )\n        )\n      )\n    )\n    (to_logits): Sequential(\n      (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n      (1): Reduce('b n d -> b d', 'mean')\n      (2): Linear(in_features=16, out_features=1, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i in tqdm(range(len(train_data)),desc=\"Training\",total=(config[\"train_shape\"][0]//config[\"bs\"])+1):\n",
    "        cat_b,con_b,tar = train_data[i]\n",
    "\n",
    "        cat_b = cat_b.long().to(device)\n",
    "        con_b = con_b.float().to(device)\n",
    "        tar = tar.long().to(device)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs, _ = model(cat_b,con_b)\n",
    "\n",
    "        print(outputs.size(),tar.size())\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs,tar)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_data) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2869 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0, 1]) torch.Size([0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/2869 [00:02<1:44:37,  2.19s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001B[39;00m\n\u001B[0;32m     11\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 12\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_number\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# We don't need gradients on to do reporting\u001B[39;00m\n\u001B[0;32m     15\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[27], line 18\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[1;34m(epoch_index, tb_writer)\u001B[0m\n\u001B[0;32m     15\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Make predictions for this batch\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m outputs, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcat_b\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcon_b\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs\u001B[38;5;241m.\u001B[39msize(),tar\u001B[38;5;241m.\u001B[39msize())\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Compute the loss and its gradients\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Academic_PC\\Sem 7\\Machine Learning\\MiniProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[8], line 477\u001B[0m, in \u001B[0;36mGatedTabTransformer.forward\u001B[1;34m(self, x_categ, x_cont)\u001B[0m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m x_categ\u001B[38;5;241m.\u001B[39mshape[\n\u001B[0;32m    474\u001B[0m            \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_categories, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124myou must pass in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_categories\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m values for your categories input\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    475\u001B[0m x_categ \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcategories_offset\n\u001B[1;32m--> 477\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_categ\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    479\u001B[0m flat_categ \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    481\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_continuous \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\Academic_PC\\Sem 7\\Machine Learning\\MiniProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[8], line 353\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    350\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeds(x)\n\u001B[0;32m    352\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m attn, ff \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 353\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    354\u001B[0m     x \u001B[38;5;241m=\u001B[39m ff(x)\n\u001B[0;32m    356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32mD:\\Academic_PC\\Sem 7\\Machine Learning\\MiniProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[8], line 9\u001B[0m, in \u001B[0;36mResidual.forward\u001B[1;34m(self, x, **kwargs)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m----> 9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(x, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m+\u001B[39m x\n",
      "File \u001B[1;32mD:\\Academic_PC\\Sem 7\\Machine Learning\\MiniProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[8], line 18\u001B[0m, in \u001B[0;36mPreNorm.forward\u001B[1;34m(self, x, **kwargs)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Academic_PC\\Sem 7\\Machine Learning\\MiniProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[8], line 325\u001B[0m, in \u001B[0;36mHeadAttention.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m    324\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheads\n\u001B[1;32m--> 325\u001B[0m     q, k, v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_qkv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mchunk(\u001B[38;5;241m3\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    326\u001B[0m     q, k, v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m t: rearrange(t, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb n (h d) -> b h n d\u001B[39m\u001B[38;5;124m'\u001B[39m, h\u001B[38;5;241m=\u001B[39mh), (q, k, v))\n\u001B[0;32m    327\u001B[0m     sim \u001B[38;5;241m=\u001B[39m einsum(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb h i d, b h j d -> b h i j\u001B[39m\u001B[38;5;124m'\u001B[39m, q, k) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale\n",
      "File \u001B[1;32mD:\\Academic_PC\\Sem 7\\Machine Learning\\MiniProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\Academic_PC\\Sem 7\\Machine Learning\\MiniProject\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(config.get(\"epoch\")):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i in tqdm(range(len(val_data)),desc=\"Validation\",total=(config[\"val_shape\"][0]//config[\"bs\"])+1):\n",
    "        cat_b,con_b,tar = val_data[i]\n",
    "\n",
    "        cat_b = cat_b.long().to(device)\n",
    "        con_b = con_b.float().to(device)\n",
    "        tar = tar.long().to(device)\n",
    "\n",
    "        voutputs = model(cat_b,con_b)\n",
    "        outputs = torch.argmax(voutputs,1).float()\n",
    "\n",
    "        vloss = loss_fn(voutputs,tar)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
