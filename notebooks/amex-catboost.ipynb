{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/gauravbrills/kaggle-fiddle/blob/main/amex/amex_catboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "id": "view-in-github"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CATBOOST AMEX"
   ],
   "metadata": {
    "id": "6S1tGBZ5grxs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-requisites"
   ],
   "metadata": {
    "id": "YsemIH1bhEV-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports and Constants"
   ],
   "metadata": {
    "id": "_RMauooTHaLw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os,random \n",
    "import tqdm \n",
    "import pandas as cudf\n",
    "import numpy as cupy \n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import joblib\n",
    "import pathlib\n",
    "import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cudf.set_option('display.max_rows', 500)\n",
    "cudf.set_option('display.max_columns', 500)\n",
    "cudf.set_option('display.width', 1000)\n",
    "\n",
    "class CFG:\n",
    "  seed = 42\n",
    "  INPUT = \"../input\"\n",
    "  TRAIN = True\n",
    "  OPTIMIZE = False\n",
    "  INFER = False\n",
    "  n_folds = 5\n",
    "  target ='target'\n",
    "  DEBUG= False \n",
    "  ADD_CAT = True\n",
    "  ADD_LAG = True \n",
    "  COMPUTE_Z = True\n",
    "  ADD_DIFF_1 = True\n",
    "  ADD_DIFF =  [1,3,5]#[3,6]\n",
    "  ADD_PCTDIFF = [1,3,6]\n",
    "  KURT = False\n",
    "  TRIM=True   \n",
    "  model_dir = \"\"\n",
    "\n",
    "path = f'{CFG.INPUT}/amex-data-integer-dtypes-parquet-format'   \n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(CFG.seed)  "
   ],
   "metadata": {
    "id": "nD6axCLvgrx3",
    "execution": {
     "iopub.status.busy": "2022-07-27T00:31:38.066564Z",
     "iopub.execute_input": "2022-07-27T00:31:38.067134Z",
     "iopub.status.idle": "2022-07-27T00:31:39.175655Z",
     "shell.execute_reply.started": "2022-07-27T00:31:38.067015Z",
     "shell.execute_reply": "2022-07-27T00:31:39.174696Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Libs 4 Feature Engg"
   ],
   "metadata": {
    "id": "QM2Lb6RdY5-h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering"
   ],
   "metadata": {
    "id": "SWPQupqBgrx4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Utils"
   ],
   "metadata": {
    "id": "svO8--Jl3LHJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def agg_df_num(df):\n",
    "    df_agg = df.groupby('customer_ID').agg(f_names)\n",
    "    df_agg.columns = [str(c[0])+'_'+str(c[1]) for c in df_agg.columns]\n",
    "    return df_agg\n",
    "\n",
    "# ====================================================\n",
    "# Get the difference  --> capture fluctuations, can capture diff(1),diff(2),diff(3) and consider adding features\n",
    "# ====================================================\n",
    "def get_difference(data, num_features,period=1): \n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in  data.groupby(['customer_ID']):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(period).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + f'_diff{period}' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "def get_pct_change(data, num_features,period=1): \n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in  data.groupby(['customer_ID']):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].pct_change(period,fill_method=None).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + f'_pct_chg{period}' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1\n",
    "\n",
    "\n",
    "def kurtosis(x):\n",
    "    if not isinstance(x, pd.Series):\n",
    "        x = pd.Series(x)\n",
    "    return pd.Series.kurtosis(x)    \n",
    " \n",
    "\n",
    "CID =\"customer_ID\"\n",
    "TIME = \"S_2\"\n",
    "TARGET = \"target\"\n",
    "def pivot_data(df, train=True):\n",
    "    cols = [c for c in df.columns if c not in [CID, TIME, TARGET]]\n",
    "    tmp = df.copy()\n",
    "    tmp['max'] = tmp.groupby([CID])[TIME].transform('max')\n",
    "    tmp['size'] = tmp.groupby([CID])[TIME].transform('size')\n",
    "    tmp['rank'] = tmp.groupby([CID])[TIME].transform('rank')\n",
    "    tmp['statement'] = (tmp['size']-tmp['rank']).astype(np.int8)\n",
    "    pivot_pd = tmp.pivot(index=CID,columns=['statement'],values=cols)\n",
    "    pivot_pd.columns = [('{0}__TE{1}'.format(*tup)) for tup in pivot_pd.columns]\n",
    "    pivot_pd = pivot_pd.reset_index()\n",
    "    return pivot_pd\n",
    "\n",
    "def agg_pct_rank_by_cat(df,main_features_last,cat_features_last):\n",
    "    df_list = [] \n",
    "    for c in cat_features_last:\n",
    "        df_agg = df[main_features_last].groupby(df[c]).transform('rank')/df[main_features_last].groupby(df[c]).transform('count')\n",
    "        df_agg.columns = [f+'_pct_rank_by_'+c for f in df_agg.columns]\n",
    "        df_list.append(df_agg.astype('float16')) \n",
    "    return pd.concat([df,pd.concat(df_list,axis=1).astype('float16')], axis=1)\n",
    "def agg_global_rank(df,main_features_last):\n",
    "    df_rank = df[main_features_last].transform('rank')\n",
    "    df_rank.columns = [s+'_global_rank' for s in df_rank.columns]\n",
    "    return pd.concat([df,(df_rank/len(df)).astype('float16')],axis=1)\n",
    "\n",
    "def agg_standardize_by_cat(df,main_features_last,cat_features_last):\n",
    "    df_list = []\n",
    "    for c in cat_features_last:\n",
    "        df_agg = df[main_features_last].groupby(df[c]).transform(lambda x: (x - x.mean()) / x.std())\n",
    "        df_agg.columns = [f+'_standardized_by_'+c for f in df_agg.columns]\n",
    "        df_list.append(df_agg.astype('float16'))\n",
    "\n",
    "    return pd.concat([df,pd.concat(df_list,axis=1).astype('float16')],axis=1)"
   ],
   "metadata": {
    "id": "a8gnTxoeYN_K",
    "execution": {
     "iopub.status.busy": "2022-07-27T00:31:39.177627Z",
     "iopub.execute_input": "2022-07-27T00:31:39.178511Z",
     "iopub.status.idle": "2022-07-27T00:31:39.203245Z",
     "shell.execute_reply.started": "2022-07-27T00:31:39.178472Z",
     "shell.execute_reply": "2022-07-27T00:31:39.202307Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MAIN FE"
   ],
   "metadata": {
    "id": "EzVtgow-3PKw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CATBOOST Params and utility functions"
   ],
   "metadata": {
    "id": "ksVFY-18grx6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class AmexMetric(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return error / (weight + 1e-38)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        # approxes - list of list-like objects (one object per      approx dimension)\n",
    "        # target - list-like object\n",
    "        # weight - list-like object, can be None\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "        \n",
    "        approx = approxes[0]\n",
    "        pred = [0 for i in range(len(target))] \n",
    "        return amex_metric(np.array(target), pred), 0"
   ],
   "metadata": {
    "id": "yD1ZqJNszRTA",
    "execution": {
     "iopub.status.busy": "2022-07-27T00:31:39.365923Z",
     "iopub.execute_input": "2022-07-27T00:31:39.368258Z",
     "iopub.status.idle": "2022-07-27T00:31:39.382014Z",
     "shell.execute_reply.started": "2022-07-27T00:31:39.368221Z",
     "shell.execute_reply": "2022-07-27T00:31:39.381087Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def cat_train(x, y, xt, yt,\n",
    "               cat_features=['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120',\n",
    "                'D_126', 'D_63', 'D_64', 'D_66', 'D_68'],#+[cat+\"_first\" for cat in cat_cols],\n",
    "                params = {\"iterations\":10000}): \n",
    "    #print(params)            \n",
    "    #params = {'iterations': 4885, 'l2_leaf_reg': 3, 'bootstrap_type': 'Bernoulli', 'max_depth': 10, 'subsample': 0.16284093343361972}\n",
    "    cat_features= [col for col in x.columns if \"__TE\" in col ] \n",
    "    print(f\"cat_features {cat_features}\")\n",
    "    model = CatBoostClassifier( random_state=CFG.seed, #nan_mode='Min',\n",
    "                                task_type=\"GPU\",\n",
    "                                devices='0:1', \n",
    "                                iterations = 10500,\n",
    "                                #learning_rate = 0.01,\n",
    "                                #used_ram_limit=2*1024*1024*1024,\n",
    "                                #pinned_memory_size=2*1024*1024*1024,\n",
    "                                depth = 9,\n",
    "                                #eval_metric= AmexMetric,\n",
    "                                **params)\n",
    "    model.fit(x, y, eval_set=[(xt, yt)], cat_features=cat_features,\n",
    "              verbose=100, early_stopping_rounds=700)\n",
    "    return model.predict_proba(xt)[:, 1],model,1"
   ],
   "metadata": {
    "id": "coOPXOi_grx7",
    "execution": {
     "iopub.status.busy": "2022-07-27T00:31:39.386355Z",
     "iopub.execute_input": "2022-07-27T00:31:39.390565Z",
     "iopub.status.idle": "2022-07-27T00:31:39.401768Z",
     "shell.execute_reply.started": "2022-07-27T00:31:39.390532Z",
     "shell.execute_reply": "2022-07-27T00:31:39.400773Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Metrics"
   ],
   "metadata": {
    "id": "eUS5KsVDgrx8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def xgb_amex(y_pred, y_true):\n",
    "    return 'amex', amex_metric_np(y_pred,y_true.get_label())\n",
    "\n",
    "# Created by https://www.kaggle.com/yunchonggan\n",
    "# https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020\n",
    "def amex_metric_np(preds: np.ndarray, target: np.ndarray) -> float:\n",
    "    indices = np.argsort(preds)[::-1]\n",
    "    preds, target = preds[indices], target[indices]\n",
    "\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_mask = cum_norm_weight <= 0.04\n",
    "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
    "\n",
    "    weighted_target = target * weight\n",
    "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    n_pos = np.sum(target)\n",
    "    n_neg = target.shape[0] - n_pos\n",
    "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
    "\n",
    "    g = gini / gini_max\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "# we still need the official metric since the faster version above is slightly off\n",
    "import pandas as pd\n",
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True"
   ],
   "metadata": {
    "id": "KiUMoni5grx8",
    "execution": {
     "iopub.status.busy": "2022-07-27T00:31:39.402991Z",
     "iopub.execute_input": "2022-07-27T00:31:39.403560Z",
     "iopub.status.idle": "2022-07-27T00:31:39.428036Z",
     "shell.execute_reply.started": "2022-07-27T00:31:39.403488Z",
     "shell.execute_reply": "2022-07-27T00:31:39.427198Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data and add feature"
   ],
   "metadata": {
    "id": "e8ZNozN7grx9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train CAT_BOOST"
   ],
   "metadata": {
    "id": "0y1TLKFDgrx_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "not_used = get_not_used()\n",
    "msgs = {}\n",
    "folds = CFG.n_folds\n",
    "score = 0\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed) \n",
    "\n",
    "def train_fn(fold,x,y,xt,yt,_params= {}):   \n",
    "    print(\"Start training\")\n",
    "    val_pred,model, bst = cat_train(x, y, xt, yt,params=_params)  \n",
    "    return val_pred,model\n",
    "\n",
    "if CFG.TRAIN:  \n",
    "  features = [col for col in train.columns if col not in  get_not_used()]\n",
    "  oof_predictions = np.zeros(len(train))\n",
    "  feature_importances = pd.DataFrame()\n",
    "  feature_importances[\"feature\"] = features\n",
    "  for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):   \n",
    "      x, y = train[features].iloc[trn_ind], train[CFG.target].iloc[trn_ind]\n",
    "      xt, yt= train[features].iloc[val_ind], train[CFG.target].iloc[val_ind]\n",
    "      cat_features= [col for col in x.columns if \"__TE\" in col ]\n",
    "      x[cat_features] = x[cat_features].values.astype(int)\n",
    "      xt[cat_features] = xt[cat_features].values.astype(int)\n",
    "      x[cat_features]= x[cat_features].fillna(-1)\n",
    "      xt[cat_features]= xt[cat_features].fillna(-1)\n",
    "      if os.path.exists(f\"{CFG.model_dir}/1cat_fold{fold}_seed{CFG.seed}.pkl\"):\n",
    "        model = joblib.load(f\"{CFG.model_dir}/1cat_fold{fold}_seed{CFG.seed}.pkl\")\n",
    "        val_pred = model.predict_proba(xt)[:, 1] \n",
    "      else: \n",
    "        val_pred,model=train_fn(fold,x,y,xt,yt)\n",
    "        joblib.dump(model, f'cat_fold{fold}_seed{CFG.seed}.pkl')\n",
    "        \n",
    "      amex_score = amex_metric(yt.values,val_pred) \n",
    "      msg = f\"Fold {fold} amex {amex_score:.4f}\"          \n",
    "      oof_predictions[val_ind] = val_pred\n",
    "      feature_importances[f\"importance_fold{fold}+1\"] = model.feature_importances_\n",
    "      print(msg)\n",
    "      score += amex_score  \n",
    "      del x,y,xt,yt; gc.collect()\n",
    "  oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[CFG.target], 'prediction': oof_predictions})\n",
    "  display(oof_df.head())\n",
    "  oof_df.to_csv(f'cat_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False)    \n",
    "  feature_importances .to_csv(f'feature_importances_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False) \n",
    "  score /= folds\n",
    "  print(f\"Average amex score: {score:.4f}\") \n",
    "      \n",
    "if CFG.INFER:\n",
    "  test_predictions = np.zeros(len(test))\n",
    "  not_used = [i for i in not_used if i in test.columns]\n",
    "  for fold  in range(CFG.n_folds):\n",
    "    model = joblib.load(f'cat_fold{fold}_seed{CFG.seed}.pkl')\n",
    "    test_pred = model.predict_proba(test[features])[:, 1]\n",
    "    test_predictions += test_pred / CFG.n_folds   \n",
    "    torch.cuda.empty_cache() \n",
    "  test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "  test_df.to_csv(f'test_cat_{CFG.n_folds}fold_seed{CFG.seed}.csv', index = False) "
   ],
   "metadata": {
    "id": "OyhJebfRt-VT",
    "outputId": "9fb8d792-64d5-4dc8-81d1-6943236b9e69",
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_not_used' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m not_used \u001B[38;5;241m=\u001B[39m \u001B[43mget_not_used\u001B[49m()\n\u001B[0;32m      2\u001B[0m msgs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m      3\u001B[0m folds \u001B[38;5;241m=\u001B[39m CFG\u001B[38;5;241m.\u001B[39mn_folds\n",
      "\u001B[1;31mNameError\u001B[0m: name 'get_not_used' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (16,9)\n",
    "plt.rcParams[\"figure.facecolor\"] = '#FFFACD'\n",
    "plt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\n",
    "plt.rcParams[\"axes.grid\"] = True \n",
    "plt.rcParams[\"grid.alpha\"] = 0.5\n",
    "plt.rcParams[\"grid.linestyle\"] = '--'\n",
    "\n",
    "feature_importances['mean_importance']=feature_importances[[f'importance_fold{fold_n}+1' for fold_n in range(CFG.n_folds)]].mean(axis=1)\n",
    "feature_importances.sort_values(by='mean_importance', ascending=False, inplace=True)\n",
    "sns.barplot(y=feature_importances['feature'][:50],x=feature_importances['mean_importance'][:50], palette='inferno')\n",
    "plt.title('Mean Feature Importance by Folds')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "crYyQvJ4wG-a",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optuna"
   ],
   "metadata": {
    "id": "SgQ8Pd9aSjaj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01moptuna\u001B[39;00m \n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptuna\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CatBoostPruningCallback\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptunaOpt\u001B[39m(model_name,t_params,n_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, callbacks\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m trial: [])):\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.integration import CatBoostPruningCallback\n",
    "\n",
    "\n",
    "def optunaOpt(model_name,t_params,n_trials=100, callbacks=(lambda trial: [])):\n",
    "    \"\"\" Best model eval util using Optuna\n",
    "    \"\"\"\n",
    "    def run(trials):\n",
    "        \"\"\" Optima trials lambda\"\"\"\n",
    "        trial_params = {param:param_fn(trials) for param,param_fn in t_params.items()}\n",
    "        if trial_params[\"bootstrap_type\"] == \"Bayesian\":\n",
    "          trial_params[\"bagging_temperature\"] =trials.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "        if trial_params[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "          trial_params[\"subsample\"] =trials.suggest_float(\"subsample\", 0.1, 1)\n",
    "        not_used = get_not_used()\n",
    "        not_used = [i for i in not_used if i in train.columns]\n",
    "        for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[CFG.target])):\n",
    "            x, y = train[features].iloc[trn_ind], train[CFG.target].iloc[trn_ind]\n",
    "            xt, yt= train[features].iloc[val_ind], train[CFG.target].iloc[val_ind]\n",
    "            val_pred,model=train_fn(fold,x,y,xt,yt,trial_params)\n",
    "            break\n",
    "\n",
    "        amex_score = amex_metric(yt.values,val_pred)\n",
    "        return amex_score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                study_name=f\"{model_name}-study\")\n",
    "    study.optimize(run, n_trials)\n",
    "    print('\\n Best Trial:')\n",
    "    print(study.best_trial)\n",
    "    print('\\n Best value')\n",
    "    print(study.best_value)\n",
    "    print('\\n Best hyperparameters:')\n",
    "    print(study.best_params)\n",
    "    return study"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 0.7932\n",
    "catb_params = {\n",
    "    #\"iterations\":lambda trial :trial.suggest_int(\"iterations\", 6000, 11000), \n",
    "    #\"learning_rate\":lambda trial :trial.suggest_loguniform(\"learning_rate\", 0.1,1.0), \n",
    "    'l2_leaf_reg' : lambda trial :trial.suggest_categorical('l2_leaf_reg',[0.2,0.5,1,3]),\n",
    "    \"boosting_type\": lambda trial :trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "    \"bootstrap_type\": lambda trial:trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\" ]\n",
    "        ),\n",
    "    #\"colsample_bylevel\": lambda trial:trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1, log=True),\n",
    "    \"depth\":lambda trial :trial.suggest_int(\"max_depth\", 7, 12),   \n",
    "}\n",
    "\n",
    "if CFG.OPTIMIZE:\n",
    "  optunaOpt(\"Catboost\",catb_params,n_trials=100, callbacks=(lambda trial: []))"
   ],
   "metadata": {
    "id": "U8JeQHrIPXjQ",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
